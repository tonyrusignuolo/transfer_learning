{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow_model_optimization.python.core.quantization.keras import quantize_layer\n",
    "from tensorflow_model_optimization.python.core.quantization.keras import quantizers\n",
    "from tensorflow_model_optimization.python.core.quantization.keras.graph_transformations import transforms\n",
    "LayerNode = transforms.LayerNode\n",
    "\n",
    "import features as features_lib\n",
    "import params\n",
    "params = params.Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "def _batch_norm(name, params):\n",
    "    def _bn_layer(layer_input):\n",
    "        return layers.BatchNormalization(\n",
    "            name=name,\n",
    "            center=params.batchnorm_center,\n",
    "            scale=params.batchnorm_scale,\n",
    "            epsilon=params.batchnorm_epsilon,\n",
    "        )(layer_input)\n",
    "\n",
    "    return _bn_layer\n",
    "\n",
    "\n",
    "def _conv(name, kernel, stride, filters, params):\n",
    "    def _conv_layer(layer_input):\n",
    "        output = layers.Conv2D(\n",
    "            name=\"{}/conv\".format(name),\n",
    "            filters=filters,\n",
    "            kernel_size=kernel,\n",
    "            strides=stride,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(layer_input)\n",
    "        output = _batch_norm(\"{}/conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/relu6\".format(name))\n",
    "        return output\n",
    "\n",
    "    return _conv_layer\n",
    "\n",
    "\n",
    "def _separable_conv(name, kernel, stride, filters, params):\n",
    "    def _separable_conv_layer(layer_input):\n",
    "        output = layers.DepthwiseConv2D(\n",
    "            name=\"{}/depthwise_conv\".format(name),\n",
    "            kernel_size=kernel,\n",
    "            strides=stride,\n",
    "            depth_multiplier=1,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(layer_input)\n",
    "        output = _batch_norm(\"{}/depthwise_conv/bn\".format(name), params)(output)\n",
    "        output = tf.nn.relu6(output, \"{}/depthwise_conv/relu6\".format(name))\n",
    "        output = layers.Conv2D(\n",
    "            name=\"{}/pointwise_conv\".format(name),\n",
    "            filters=filters,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=1,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(output)\n",
    "        output = _batch_norm(\"{}/pointwise_conv/bn\".format(name), params)(output)\n",
    "        output = tf.nn.relu6(output, \"{}/pointwise_conv/relu6\".format(name))\n",
    "        return output\n",
    "\n",
    "    return _separable_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_YAMNET_LAYER_DEFS = [\n",
    "    # (layer_function, kernel, stride, num_filters)\n",
    "    (_conv, [3, 3], 2, 32),\n",
    "    (_separable_conv, [3, 3], 1, 64),\n",
    "    (_separable_conv, [3, 3], 2, 128),\n",
    "    (_separable_conv, [3, 3], 1, 128),\n",
    "    (_separable_conv, [3, 3], 2, 256),\n",
    "    (_separable_conv, [3, 3], 1, 256),\n",
    "    (_separable_conv, [3, 3], 2, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 2, 1024),\n",
    "    (_separable_conv, [3, 3], 1, 1024),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "waveform = layers.Input(\n",
    "    batch_shape=(params.min_num_samples,),\n",
    "    dtype=tf.float32,\n",
    "    name=\"waveform_binary 0\"\n",
    ")\n",
    "\n",
    "# magnitude spectrogram\n",
    "window_length_samples = int(round(params.sample_rate * params.stft_window_seconds))\n",
    "hop_length_samples = int(round(params.sample_rate * params.stft_hop_seconds))\n",
    "fft_length = 2 ** int(np.ceil(np.log(window_length_samples) / np.log(2.0)))\n",
    "framed_signal = tf.signal.frame(waveform, window_length_samples, hop_length_samples)\n",
    "hann_window = tf.reshape(\n",
    "    tf.constant(\n",
    "        (0.5 - 0.5 * np.cos(2 * np.pi * np.arange(0, 1.0, 1.0 / window_length_samples))).astype(np.float32),\n",
    "        name='hann_window'\n",
    "    ),\n",
    "    [1, window_length_samples]\n",
    ")\n",
    "windowed_signal = framed_signal * hann_window\n",
    "\n",
    "# rfft\n",
    "signal_frame_length = tf.shape(windowed_signal)[-1]\n",
    "half_pad = (fft_length - signal_frame_length) // 2\n",
    "padded_windowed_signal = tf.pad(\n",
    "    windowed_signal,\n",
    "    [\n",
    "        # Don't add any padding in the frame dimension.\n",
    "        [0, 0],\n",
    "        # Pad before and after the signal within each frame.\n",
    "        [half_pad, fft_length - signal_frame_length - half_pad]\n",
    "    ],\n",
    "    mode='CONSTANT',\n",
    "    constant_values=0.0\n",
    ")\n",
    "reshaped_padded_windowed_signal = tf.reshape(\n",
    "    padded_windowed_signal,\n",
    "    [\n",
    "        padded_windowed_signal.shape[0],\n",
    "        1,\n",
    "        padded_windowed_signal.shape[1],\n",
    "    ]\n",
    ")\n",
    "# rdft = tf.signal.rfft2d(reshaped_padded_windowed_signal, [1,fft_length])\n",
    "rdft = tf.keras.layers.Lambda(lambda x: tf.signal.rfft2d(x, [1,fft_length]))(reshaped_padded_windowed_signal)\n",
    "\n",
    "# possibly a reshape in here\n",
    "reshaped_rdft = tf.reshape(\n",
    "    rdft,\n",
    "    [\n",
    "        rdft.shape[0],\n",
    "        rdft.shape[2],\n",
    "    ]\n",
    ")\n",
    "complex_abs = tf.math.abs(reshaped_rdft)\n",
    "\n",
    "# magnitude_spectrogram = _tflite_stft_magnitude(\n",
    "#     signal=waveform,\n",
    "#     frame_length=window_length_samples,\n",
    "#     frame_step=hop_length_samples,\n",
    "#     fft_length=fft_length\n",
    "# )\n",
    "\n",
    "\n",
    "# linear mel weight matrix\n",
    "num_spectrogram_bins = fft_length // 2 + 1\n",
    "linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(\n",
    "    num_mel_bins=params.mel_bands,\n",
    "    num_spectrogram_bins=num_spectrogram_bins,\n",
    "    sample_rate=params.sample_rate,\n",
    "    lower_edge_hertz=params.mel_min_hz,\n",
    "    upper_edge_hertz=params.mel_max_hz\n",
    ")\n",
    "\n",
    "# mel spectrogram\n",
    "mel_spectrogram = tf.matmul(\n",
    "    complex_abs,\n",
    "    linear_to_mel_weight_matrix\n",
    ")\n",
    "\n",
    "\n",
    "# log mel spectrogram\n",
    "log_mel_spectrogram = tf.math.log(mel_spectrogram + params.log_offset)\n",
    "\n",
    "# features\n",
    "# spectrogram_hop_length_samples = int(round(params.sample_rate * params.stft_hop_seconds))\n",
    "# spectrogram_sample_rate = params.sample_rate / spectrogram_hop_length_samples\n",
    "# patch_window_length_samples = int(round(spectrogram_sample_rate * params.patch_window_seconds))\n",
    "# patch_hop_length_samples = int(round(spectrogram_sample_rate * params.patch_hop_seconds))\n",
    "# # features = tf.signal.frame(\n",
    "# #     signal=log_mel_spectrogram,\n",
    "# #     frame_length=patch_window_length_samples,\n",
    "# #     frame_step=patch_hop_length_samples,\n",
    "# #     axis=0\n",
    "# # )\n",
    "reshaped_lms = tf.reshape(\n",
    "    log_mel_spectrogram,\n",
    "    [\n",
    "        1,\n",
    "        log_mel_spectrogram.shape[0],\n",
    "        log_mel_spectrogram.shape[1],\n",
    "    ]\n",
    ")\n",
    "# quant_rlms = tf.quantization.quantize(\n",
    "#     reshaped_lms,\n",
    "#     -5400,\n",
    "#     5400,\n",
    "#     tf.quint16,\n",
    "#     name='quant_rlms'\n",
    "# )[0]\n",
    "# def replacement(match_layer):\n",
    "#     quant_layer = quantize_layer.QuantizeLayer(\n",
    "#         quantizers.AllValuesQuantizer(\n",
    "#             num_bits=8, per_axis=False, symmetric=False, narrow_range=False))\n",
    "#     layer_config = tf.keras.layers.serialize(quant_layer)\n",
    "#     layer_config['name'] = quant_layer.name\n",
    "\n",
    "#     quant_layer_node = LayerNode(\n",
    "#         layer_config,\n",
    "#         input_layers=[match_layer])\n",
    "\n",
    "#     return quant_layer_node\n",
    "\n",
    "# quant_rlms = replacement(reshaped_lms)\n",
    "quant_rlms = reshaped_lms\n",
    "reshaped_qrlms = tf.reshape(\n",
    "    quant_rlms,\n",
    "    [\n",
    "        quant_rlms.shape[0],\n",
    "        quant_rlms.shape[1],\n",
    "        quant_rlms.shape[2],\n",
    "        1\n",
    "    ]\n",
    ")\n",
    "net = tf.split(\n",
    "    reshaped_qrlms,\n",
    "    1,\n",
    "    axis=0\n",
    ")[0]\n",
    "# prep conv mobilenet\n",
    "# net = layers.Reshape(\n",
    "#     (params.patch_frames, params.patch_bands, 1),\n",
    "#     input_shape=(params.patch_frames, params.patch_bands),\n",
    "# )(split_rqlms)\n",
    "\n",
    "# mobilenet\n",
    "for (i, (layer_fun, kernel, stride, filters)) in enumerate(_YAMNET_LAYER_DEFS):\n",
    "    net = layer_fun(\n",
    "        \"layer{}\".format(i + 1),\n",
    "        kernel,\n",
    "        stride,\n",
    "        filters,\n",
    "        params\n",
    "    )(net)\n",
    "\n",
    "embeddings = layers.GlobalAveragePooling2D()(net)\n",
    "logits = layers.Dense(units=params.num_classes, use_bias=True)(embeddings)\n",
    "predictions = layers.Activation(activation=params.classifier_activation)(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet = Model(\n",
    "    name=\"yamnet_test\",\n",
    "    inputs=waveform,\n",
    "    # outputs=[quant_rlms]\n",
    "    outputs = [predictions]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and run inference on test data\n",
    "yamnet.save(\n",
    "    \"models/3/tf\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(yamnet.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_model(yamnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.layers.core.TFOpLambda object at 0x00000226A0B5E630>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A0B5E978>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7B70B38>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A0B41160>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7B74B70>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7B74470>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7BCA1D0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7BCFBE0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A0851400>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7BD7668>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7BD74E0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7BCF668>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7BE6C18>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7BDC2B0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7BDCE80>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7BF9208>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7BF2668>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7BF2588>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7BD7278>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7BCAF28>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7BCABA8>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7BC4898>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A0B5E080>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A0B5E048>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A0B2D940>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C07B38>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C07668>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C0C9B0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C12CF8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C18400>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C18A20>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C216A0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A0B415F8>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C27CC0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C210F0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C21C18>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C37940>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C270F0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C37A90>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C442B0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C3E2E8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C44F28>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C2E048>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C52F60>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C4BE48>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C5ABE0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C62E80>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C62828>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C696D8>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C776A0>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C771D0>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C7E4E0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C869B0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C86128>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C86518>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C8DF28>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C8D2B0>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C95C18>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C8D0B8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C86550>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C62390>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C71978>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C7E0B8>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C449E8>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7C375F8>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7C37160>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C71A90>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A0B41B00>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7C0C160>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A0B5ECC0>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7BC4518>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7BC4BE0>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7C4BE10>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7CADA90>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7CA7A90>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7CB8898>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7CBDBE0>\n",
      "<keras.layers.convolutional.DepthwiseConv2D object at 0x00000226A7CBD4E0>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7CBD550>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7CCC588>\n",
      "<keras.layers.convolutional.Conv2D object at 0x00000226A7CC4518>\n",
      "<keras.layers.normalization.batch_normalization.BatchNormalization object at 0x00000226A7CD3668>\n",
      "<keras.layers.core.TFOpLambda object at 0x00000226A7CCCE48>\n",
      "<keras.layers.pooling.GlobalAveragePooling2D object at 0x00000226A0B5E9B0>\n",
      "<keras.layers.core.Dense object at 0x00000226A0B5E898>\n",
      "<keras.layers.core.Activation object at 0x00000226A7CDBC18>\n"
     ]
    }
   ],
   "source": [
    "names = set()\n",
    "for i, layer in enumerate(yamnet.layers):\n",
    "    # if \"layer1/conv/bn/FusedBatchNormV3\" in layer.name:\n",
    "    if i >=18: # and i <= 103:\n",
    "        print(layer)\n",
    "        names.add(layer.name)\n",
    "    # print(f'i: {i}\\n name: {layer.name}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d8t = tfmot.quantization.keras.default_8bit.default_8bit_transforms\n",
    "transforms = [\n",
    "    d8t.InputLayerQuantize,\n",
    "    d8t.InputLayerQuantize,\n",
    "    d8t.Conv2DBatchNormQuantize,\n",
    "    d8t.InputLayerQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.SeparableConvQuantize,\n",
    "    d8t.InputLayerQuantize,\n",
    "    d8t.InputLayerQuantize,\n",
    "    d8t.InputLayerQuantize\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfmot model transformer\n",
    "mt = tfmot.quantization.keras.graph_transformations.model_transformer.ModelTransformer(\n",
    "    yamnet,\n",
    "    transforms,\n",
    "    list(names)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7dae1ad3a60b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\graph_transformations\\model_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m       \u001b[0mmatch_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    583\u001b[0m         \u001b[1;31m# A transform may find multiple instances of a pattern in the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# Keep finding and replacing till done.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not iterable"
     ]
    }
   ],
   "source": [
    "mt.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_quant_to_layers(layer):\n",
    "    if layer.name in names:\n",
    "        return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_model = tf.keras.models.clone_model(\n",
    "    yamnet,\n",
    "    clone_function=apply_quant_to_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
    "base_data_path = './datasets/ESC-50-master/audio/'\n",
    "\n",
    "pd_data = pd.read_csv(esc50_csv)\n",
    "pd_data.head()\n",
    "\n",
    "my_classes = ['dog', 'cat']\n",
    "map_class_to_id = {'dog':0, 'cat':1}\n",
    "\n",
    "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
    "\n",
    "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
    "filtered_pd = filtered_pd.assign(target=class_id)\n",
    "\n",
    "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
    "filtered_pd = filtered_pd.assign(filename=full_path)\n",
    "\n",
    "filtered_pd.head(10)\n",
    "\n",
    "filenames = filtered_pd['filename']\n",
    "targets = filtered_pd['target']\n",
    "folds = filtered_pd['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "  \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "  file_contents = tf.io.read_file(filename)\n",
    "  wav, sample_rate = tf.audio.decode_wav(\n",
    "        file_contents,\n",
    "        desired_channels=1)\n",
    "  wav = tf.squeeze(wav, axis=-1)\n",
    "  sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "  wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "  return wav\n",
    "\n",
    "@tf.function\n",
    "def frame_16k_mono(filename):\n",
    "  wav = load_wav_16k_mono(filename)\n",
    "  frames = tf.signal.frame(wav, 15600, 15600)\n",
    "  return frames\n",
    "\n",
    "    \n",
    "def load_frames_for_map(filename, label, fold):\n",
    "  frames = frame_16k_mono(filename)\n",
    "  return (\n",
    "    frames,\n",
    "    label,\n",
    "    fold\n",
    ")\n",
    "\n",
    "def unbatch_frames(frames, label, fold):\n",
    "    # num_frames = reduce((lambda x, y: x* y), frames.shape[0:-1])\n",
    "    num_frames = 5\n",
    "    frames = tf.reshape(frames,[num_frames, 15600])\n",
    "    return (\n",
    "        frames, \n",
    "        tf.repeat(label, num_frames),\n",
    "        tf.repeat(fold, num_frames)\n",
    "    )\n",
    "\n",
    "main_ds = main_ds.map(load_frames_for_map)\n",
    "main_ds = main_ds.map(unbatch_frames).unbatch()\n",
    "\n",
    "# split the data\n",
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda frame, label, fold: fold < 4)\n",
    "val_ds = cached_ds.filter(lambda frame, label, fold: fold == 4)\n",
    "test_ds = cached_ds.filter(lambda frame, label, fold: fold == 5)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda frame, label, fold: (frame, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)\n",
    "\n",
    "# quantization of weights and activations\n",
    "def representative_dataset():\n",
    "    for frame, label in train_ds.take(100):\n",
    "        yield [frame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(yamnet)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "with open('./models/3/ye1.tflite', 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "062744ba756b440054f685d9f8f630c269074acb09f79db40ed3ef0e5e69481c"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('audio3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
