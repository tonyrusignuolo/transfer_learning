{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import features as features_lib\n",
    "import params\n",
    "params = params.Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "def _batch_norm(name, params):\n",
    "    def _bn_layer(layer_input):\n",
    "        return layers.BatchNormalization(\n",
    "            name=name,\n",
    "            center=params.batchnorm_center,\n",
    "            scale=params.batchnorm_scale,\n",
    "            epsilon=params.batchnorm_epsilon,\n",
    "        )(layer_input)\n",
    "\n",
    "    return _bn_layer\n",
    "\n",
    "\n",
    "def _conv(name, kernel, stride, filters, params):\n",
    "    def _conv_layer(layer_input):\n",
    "        output = layers.Conv2D(\n",
    "            name=\"{}/conv\".format(name),\n",
    "            filters=filters,\n",
    "            kernel_size=kernel,\n",
    "            strides=stride,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(layer_input)\n",
    "        output = _batch_norm(\"{}/conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/relu6\".format(name))\n",
    "        return output\n",
    "\n",
    "    return _conv_layer\n",
    "\n",
    "\n",
    "def _separable_conv(name, kernel, stride, filters, params):\n",
    "    def _separable_conv_layer(layer_input):\n",
    "        output = layers.DepthwiseConv2D(\n",
    "            name=\"{}/depthwise_conv\".format(name),\n",
    "            kernel_size=kernel,\n",
    "            strides=stride,\n",
    "            depth_multiplier=1,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(layer_input)\n",
    "        output = _batch_norm(\"{}/depthwise_conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/depthwise_conv/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/depthwise_conv/relu6\".format(name))\n",
    "        output = layers.Conv2D(\n",
    "            name=\"{}/pointwise_conv\".format(name),\n",
    "            filters=filters,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=1,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(output)\n",
    "        output = _batch_norm(\"{}/pointwise_conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/pointwise_conv/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/pointwise_conv/relu6\".format(name))\n",
    "        return output\n",
    "\n",
    "    return _separable_conv_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_YAMNET_LAYER_DEFS = [\n",
    "    # (layer_function, kernel, stride, num_filters)\n",
    "    (_conv, [3, 3], 2, 32),\n",
    "    (_separable_conv, [3, 3], 1, 64),\n",
    "    (_separable_conv, [3, 3], 2, 128),\n",
    "    (_separable_conv, [3, 3], 1, 128),\n",
    "    (_separable_conv, [3, 3], 2, 256),\n",
    "    (_separable_conv, [3, 3], 1, 256),\n",
    "    (_separable_conv, [3, 3], 2, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 2, 1024),\n",
    "    (_separable_conv, [3, 3], 1, 1024),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "waveform = layers.Input(\n",
    "    batch_shape=(params.min_num_samples,),\n",
    "    dtype=tf.float32,\n",
    "    name=\"waveform_binary 0\"\n",
    ")\n",
    "(\n",
    "    log_mel_spectrogram,\n",
    "    features,\n",
    ") = features_lib.waveform_to_log_mel_spectrogram_patches(waveform, params)\n",
    "\n",
    "# quant_features = tf.quantization.quantize(\n",
    "#     features,\n",
    "#     -2700.0,\n",
    "#     2700.0,\n",
    "#     tf.dtypes.quint8,\n",
    "#     mode='MIN_COMBINED',\n",
    "#     round_mode='HALF_AWAY_FROM_ZERO',\n",
    "#     name=None,\n",
    "#     narrow_range=False,\n",
    "#     axis=None,\n",
    "#     ensure_minimum_range=0.01\n",
    "# )\n",
    "\n",
    "\n",
    "# features_casted = tf.cast(features, tf.int8)\n",
    "r1 = net = layers.Reshape(\n",
    "    (params.patch_frames, params.patch_bands, 1),\n",
    "    input_shape=(params.patch_frames, params.patch_bands),\n",
    ")(features)\n",
    "# )(quant_features)\n",
    "\n",
    "for (i, (layer_fun, kernel, stride, filters)) in enumerate(_YAMNET_LAYER_DEFS):\n",
    "    net = layer_fun(\"layer{}\".format(i + 1), kernel, stride, filters, params)(net)\n",
    "embeddings = layers.GlobalAveragePooling2D()(net)\n",
    "logits = layers.Dense(units=params.num_classes, use_bias=True)(embeddings)\n",
    "predictions = layers.Activation(activation=params.classifier_activation)(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfmot quantizers test\n",
    "tfmot.quantization.keras.quantizers.AllValuesQuantizer(\n",
    "    8,\n",
    "    True,\n",
    "    True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet = Model(\n",
    "    name=\"yamnet_test\",\n",
    "    inputs=waveform,\n",
    "    outputs=[predictions]\n",
    ")\n",
    "yamnet.load_weights('yamnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The added layer must be an instance of class Layer. Found: KerasTensor(type_spec=TensorSpec(shape=(1, 96, 64, 1), dtype=tf.float32, name=None), name='reshape_1/Reshape:0', description=\"created by layer 'reshape_1'\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d1ef3b5030c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     [\n\u001b[0;32m     10\u001b[0m         \u001b[0mdummy_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mr1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     ],\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# name=\"yamnet_cnn\",\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, layers, name)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mlayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\engine\\sequential.py\u001b[0m in \u001b[0;36madd\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    175\u001b[0m       raise TypeError('The added layer must be '\n\u001b[0;32m    176\u001b[0m                       \u001b[1;34m'an instance of class Layer. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                       'Found: ' + str(layer))\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_no_legacy_layers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: The added layer must be an instance of class Layer. Found: KerasTensor(type_spec=TensorSpec(shape=(1, 96, 64, 1), dtype=tf.float32, name=None), name='reshape_1/Reshape:0', description=\"created by layer 'reshape_1'\")"
     ]
    }
   ],
   "source": [
    "# try to isolate just the cnn\n",
    "dummy_input = layers.Input(\n",
    "    batch_shape=(1, 96, 64),\n",
    "    dtype=tf.float32,\n",
    "    name=\"dummy input\"\n",
    ")\n",
    "# r1(dummy_input)\n",
    "cnn = tf.keras.Sequential(\n",
    "    [\n",
    "        dummy_input,\n",
    "        r1\n",
    "    ],\n",
    "    # name=\"yamnet_cnn\",\n",
    "    # inputs=dummy_input,\n",
    "    # outputs=[logits]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) waveform_binary 0 with unsupported characters which will be renamed to placeholder in the SavedModel.\n"
     ]
    },
    {
     "ename": "ConverterError",
     "evalue": "C:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\keras\\engine\\functional.py:415:0: error: 'tf.QuantizeV2' op is neither a custom op nor a flex op\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\keras\\engine\\base_layer.py:1037:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_keras_util.py:194:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3308:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3066:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:760:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:1213:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\keras\\engine\\functional.py:415:0: note: Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: QuantizeV2\nDetails:\n\ttf.QuantizeV2(tensor<1x96x64xf32>, tensor<f32>, tensor<f32>) -> (tensor<1x96x64x!tf.quint8>, tensor<f32>, tensor<f32>) : {T = !tf.quint8, axis = -1 : i64, device = \"\", ensure_minimum_range = 0.00999999977 : f32, mode = \"MIN_COMBINED\", narrow_range = false, round_mode = \"HALF_AWAY_FROM_ZERO\"}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConverterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-201da713a1cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# converter = tf.lite.TFLiteConverter.from_keras_model(yamnet)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m# save to file called me/yamnet.tflite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# with open('models/3/me/yamnet_quant.tflite', 'wb') as f:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 729\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_and_export_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvert_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    730\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36m_convert_and_export_metrics\u001b[1;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_save_conversion_params_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[0melapsed_time_ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m     return super(TFLiteKerasModelConverterV2,\n\u001b[1;32m-> 1129\u001b[1;33m                  self).convert(graph_def, input_tensors, output_tensors)\n\u001b[0m\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[0;32m    899\u001b[0m         \u001b[0minput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[0moutput_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m         **converter_kwargs)\n\u001b[0m\u001b[0;32m    902\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m     return self._optimize_tflite_model(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m           \u001b[0mreport_error_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# Re-throws the exception.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[0mreport_error_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\convert_phase.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mConverterError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\u001b[0m in \u001b[0;36mtoco_convert_impl\u001b[1;34m(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)\u001b[0m\n\u001b[0;32m    798\u001b[0m       \u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m       \u001b[0mdebug_info_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug_info_str\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m       enable_mlir_converter=enable_mlir_converter)\n\u001b[0m\u001b[0;32m    801\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\convert.py\u001b[0m in \u001b[0;36mtoco_convert_protos\u001b[1;34m(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)\u001b[0m\n\u001b[0;32m    311\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0merror_data\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_metrics_wrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve_collected_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[0mconverter_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mconverter_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m   return _run_toco_binary(model_flags_str, toco_flags_str, input_data_str,\n",
      "\u001b[1;31mConverterError\u001b[0m: C:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\keras\\engine\\functional.py:415:0: error: 'tf.QuantizeV2' op is neither a custom op nor a flex op\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\keras\\engine\\base_layer.py:1037:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\lite\\python\\tflite_keras_util.py:194:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:668:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1007:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3308:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3463:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3066:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:760:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:1213:0: note: called from\nC:\\Users\\tonyr\\Anaconda3\\envs\\audio2\\lib\\site-packages\\keras\\engine\\functional.py:415:0: note: Error code: ERROR_NEEDS_FLEX_OPS\n<unknown>:0: error: failed while converting: 'main': \nSome ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select \nTF Select ops: QuantizeV2\nDetails:\n\ttf.QuantizeV2(tensor<1x96x64xf32>, tensor<f32>, tensor<f32>) -> (tensor<1x96x64x!tf.quint8>, tensor<f32>, tensor<f32>) : {T = !tf.quint8, axis = -1 : i64, device = \"\", ensure_minimum_range = 0.00999999977 : f32, mode = \"MIN_COMBINED\", narrow_range = false, round_mode = \"HALF_AWAY_FROM_ZERO\"}\n\n"
     ]
    }
   ],
   "source": [
    "# convert to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(yamnet)\n",
    "tflite_model = converter.convert()\n",
    "# save to file called me/yamnet.tflite\n",
    "# with open('models/3/me/yamnet_quant.tflite', 'wb') as f:\n",
    "with open('models/3/me/features.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcc6f3ac80864b41a09a5597412753a4223d9f68d2603bb7195a8422dc11dcbe"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('audio2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
