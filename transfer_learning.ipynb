{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import pandas as pd\n",
    "import tensorflow_io as tfio\n",
    "\n",
    "import features as features_lib\n",
    "import params\n",
    "params = params.Params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "def _batch_norm(name, params):\n",
    "    def _bn_layer(layer_input):\n",
    "        return layers.BatchNormalization(\n",
    "            name=name,\n",
    "            center=params.batchnorm_center,\n",
    "            scale=params.batchnorm_scale,\n",
    "            epsilon=params.batchnorm_epsilon,\n",
    "        )(layer_input)\n",
    "\n",
    "    return _bn_layer\n",
    "\n",
    "\n",
    "def _conv(name, kernel, stride, filters, params):\n",
    "    def _conv_layer(layer_input):\n",
    "        output = layers.Conv2D(\n",
    "            name=\"{}/conv\".format(name),\n",
    "            filters=filters,\n",
    "            kernel_size=kernel,\n",
    "            strides=stride,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(layer_input)\n",
    "        output = _batch_norm(\"{}/conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/relu6\".format(name))\n",
    "        return output\n",
    "\n",
    "    return _conv_layer\n",
    "\n",
    "\n",
    "def _separable_conv(name, kernel, stride, filters, params):\n",
    "    def _separable_conv_layer(layer_input):\n",
    "        output = layers.DepthwiseConv2D(\n",
    "            name=\"{}/depthwise_conv\".format(name),\n",
    "            kernel_size=kernel,\n",
    "            strides=stride,\n",
    "            depth_multiplier=1,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(layer_input)\n",
    "        output = _batch_norm(\"{}/depthwise_conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/depthwise_conv/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/depthwise_conv/relu6\".format(name))\n",
    "        output = layers.Conv2D(\n",
    "            name=\"{}/pointwise_conv\".format(name),\n",
    "            filters=filters,\n",
    "            kernel_size=(1, 1),\n",
    "            strides=1,\n",
    "            padding=params.conv_padding,\n",
    "            use_bias=False,\n",
    "            activation=None,\n",
    "        )(output)\n",
    "        output = _batch_norm(\"{}/pointwise_conv/bn\".format(name), params)(output)\n",
    "        # output = layers.ReLU(name=\"{}/pointwise_conv/relu\".format(name))(output)\n",
    "        output = tf.nn.relu6(output, \"{}/pointwise_conv/relu6\".format(name))\n",
    "        return output\n",
    "\n",
    "    return _separable_conv_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_YAMNET_LAYER_DEFS = [\n",
    "    # (layer_function, kernel, stride, num_filters)\n",
    "    (_conv, [3, 3], 2, 32),\n",
    "    (_separable_conv, [3, 3], 1, 64),\n",
    "    (_separable_conv, [3, 3], 2, 128),\n",
    "    (_separable_conv, [3, 3], 1, 128),\n",
    "    (_separable_conv, [3, 3], 2, 256),\n",
    "    (_separable_conv, [3, 3], 1, 256),\n",
    "    (_separable_conv, [3, 3], 2, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 1, 512),\n",
    "    (_separable_conv, [3, 3], 2, 1024),\n",
    "    (_separable_conv, [3, 3], 1, 1024),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "waveform = layers.Input(\n",
    "    batch_shape=(params.min_num_samples,),\n",
    "    dtype=tf.float32,\n",
    "    name=\"waveform_binary 0\"\n",
    ")\n",
    "(\n",
    "    log_mel_spectrogram,\n",
    "    features,\n",
    ") = features_lib.waveform_to_log_mel_spectrogram_patches(waveform, params)\n",
    "\n",
    "# quant_features = tf.quantization.quantize(\n",
    "#     features,\n",
    "#     -2700.0,\n",
    "#     2700.0,\n",
    "#     tf.dtypes.quint8,\n",
    "#     mode='MIN_COMBINED',\n",
    "#     round_mode='HALF_AWAY_FROM_ZERO',\n",
    "#     name=None,\n",
    "#     narrow_range=False,\n",
    "#     axis=None,\n",
    "#     ensure_minimum_range=0.01\n",
    "# )\n",
    "\n",
    "\n",
    "# features_casted = tf.cast(features, tf.int8)\n",
    "r1 = net = layers.Reshape(\n",
    "    (params.patch_frames, params.patch_bands, 1),\n",
    "    input_shape=(params.patch_frames, params.patch_bands),\n",
    ")(features)\n",
    "# )(quant_features)\n",
    "\n",
    "for (i, (layer_fun, kernel, stride, filters)) in enumerate(_YAMNET_LAYER_DEFS):\n",
    "    net = layer_fun(\"layer{}\".format(i + 1), kernel, stride, filters, params)(net)\n",
    "embeddings = layers.GlobalAveragePooling2D()(net)\n",
    "logits = layers.Dense(units=params.num_classes, use_bias=True)(embeddings)\n",
    "predictions = layers.Activation(activation=params.classifier_activation)(logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfmot quantizers test\n",
    "# tfmot.quantization.keras.quantizers.AllValuesQuantizer(\n",
    "#     8,\n",
    "#     True,\n",
    "#     True,\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet = Model(\n",
    "    name=\"yamnet_test\",\n",
    "    inputs=waveform,\n",
    "    outputs=[predictions]\n",
    ")\n",
    "yamnet.load_weights('yamnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b02b9beee65c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfmot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myamnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36mquantize_model\u001b[1;34m(to_quantize)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m   \u001b[0mannotated_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantize_annotate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_quantize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mquantize_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mannotated_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\metrics.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     72\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FAILURE_LABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\keras\\metrics.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_gauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMonitorBoolGauge\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SUCCESS_LABEL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36mquantize_apply\u001b[1;34m(model, scheme)\u001b[0m\n\u001b[0;32m    448\u001b[0m   \u001b[1;31m# stores relevant quantization information in a map.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m   (unwrapped_model, layer_quantize_map,\n\u001b[1;32m--> 450\u001b[1;33m    requires_output_quantize) = _extract_original_model(model_copy)\n\u001b[0m\u001b[0;32m    451\u001b[0m   \u001b[1;31m# Model cloning excludes input layers. Add input layers into the map\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m   \u001b[1;31m# since they need to be matched for patterns as well.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36m_extract_original_model\u001b[1;34m(model_to_unwrap)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     unwrapped_model = keras.models.clone_model(\n\u001b[1;32m--> 382\u001b[1;33m         model_to_unwrap, input_tensors=None, clone_function=_unwrap)\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0munwrapped_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_quantize_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_output_quantize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mclone_model\u001b[1;34m(model, input_tensors, clone_function)\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m       return _clone_functional_model(\n\u001b[1;32m--> 452\u001b[1;33m           model, input_tensors=input_tensors, layer_fn=clone_function)\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m_clone_functional_model\u001b[1;34m(model, input_tensors, layer_fn)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m   model_configs, created_layers = _clone_layers_and_model_config(\n\u001b[1;32m--> 192\u001b[1;33m       model, new_input_layers, layer_fn)\n\u001b[0m\u001b[0;32m    193\u001b[0m   \u001b[1;31m# Reconstruct model from the config, using the cloned layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m   input_tensors, output_tensors, created_layers = (\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m_clone_layers_and_model_config\u001b[1;34m(model, input_layers, layer_fn)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m   config = functional.get_network_config(\n\u001b[1;32m--> 245\u001b[1;33m       model, serialize_layer_fn=_copy_layer)\n\u001b[0m\u001b[0;32m    246\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreated_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\engine\\functional.py\u001b[0m in \u001b[0;36mget_network_config\u001b[1;34m(network, serialize_layer_fn)\u001b[0m\n\u001b[0;32m   1347\u001b[0m           \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1348\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1349\u001b[1;33m       \u001b[0mlayer_config\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mserialize_layer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1350\u001b[0m       \u001b[0mlayer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m       \u001b[0mlayer_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'inbound_nodes'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_inbound_nodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36m_copy_layer\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    239\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m       \u001b[0mcreated_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow_model_optimization\\python\\core\\quantization\\keras\\quantize.py\u001b[0m in \u001b[0;36m_unwrap\u001b[1;34m(layer)\u001b[0m\n\u001b[0;32m    368\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mnode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m         \u001b[0minbound_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m         if len(inbound_layers) == 1 and not isinstance(\n\u001b[0;32m    372\u001b[0m             inbound_layers[0], quantize_annotate_mod.QuantizeAnnotate):\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\engine\\node.py\u001b[0m in \u001b[0;36minbound_layers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     inbound_layers = tf.nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[1;32m--> 257\u001b[1;33m                                         self.call_args[0])\n\u001b[0m\u001b[0;32m    258\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\keras\\engine\\node.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m     inbound_layers = tf.nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[0;32m    257\u001b[0m                                         self.call_args[0])\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tonyr\\Anaconda3\\envs\\audio3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy_ops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnp_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m         np_config.enable_numpy_behavior()\"\"\".format(type(self).__name__, name))\n\u001b[1;32m--> 401\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute '_keras_history'"
     ]
    }
   ],
   "source": [
    "tfmot.quantization.keras.quantize_model(yamnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = yamnet.get_weights()\n",
    "print(len(weights_list))\n",
    "for i in range(len(weights_list)):\n",
    "    print(weights_list[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(yamnet.layers)):\n",
    "    layer = yamnet.layers[i]\n",
    "    print(f'i: {i}, layer: {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_list[1].shape\n",
    "for weight in weights_list:\n",
    "    print(weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Weights from TFLite Edge Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yamnet_edge = tf.lite.Interpreter(model_path=\"models/yamnet/tfhub/cpu.tflite\")\n",
    "# yamnet_edge = tf.lite.Interpreter(model_path=\"models/yamnet/tflite/yamnet.tflite\")\n",
    "yamnet_edge.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yamnet_edge.allocate_tensors()\n",
    "ye_deets = yamnet_edge.get_tensor_details()\n",
    "tensors = []\n",
    "for d in ye_deets:\n",
    "    i = d['index']\n",
    "    try:\n",
    "        tensor = yamnet_edge.tensor(i)()\n",
    "        tensors.append(tensor)\n",
    "        # tensor = yamnet_edge.get_tensor(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15600,)\n",
      "(1,)\n",
      "(1,)\n",
      "(1,)\n",
      "(15600,)\n",
      "(2,)\n",
      "(195, 80)\n",
      "(96, 5)\n",
      "(96, 5, 80)\n",
      "(2,)\n",
      "(400,)\n",
      "(2, 2)\n",
      "(3,)\n",
      "(96, 1, 512)\n",
      "(2,)\n",
      "(96, 1, 257)\n",
      "(2,)\n",
      "(96, 257)\n",
      "(96, 257)\n",
      "(64, 257)\n",
      "(64,)\n",
      "(96, 64)\n",
      "(96, 64)\n",
      "(3,)\n",
      "(1, 96, 64)\n",
      "(1, 96, 64)\n",
      "(4,)\n",
      "(1, 96, 64, 1)\n",
      "()\n",
      "(1, 96, 64, 1)\n",
      "(32, 3, 3, 1)\n",
      "(32,)\n",
      "(1, 48, 32, 32)\n",
      "(1, 3, 3, 32)\n",
      "(32,)\n",
      "(1, 48, 32, 32)\n",
      "(64, 1, 1, 32)\n",
      "(64,)\n",
      "(1, 48, 32, 64)\n",
      "(1, 3, 3, 64)\n",
      "(64,)\n",
      "(1, 24, 16, 64)\n",
      "(128, 1, 1, 64)\n",
      "(128,)\n",
      "(1, 24, 16, 128)\n",
      "(1, 3, 3, 128)\n",
      "(128,)\n",
      "(1, 24, 16, 128)\n",
      "(128, 1, 1, 128)\n",
      "(128,)\n",
      "(1, 24, 16, 128)\n",
      "(1, 3, 3, 128)\n",
      "(128,)\n",
      "(1, 12, 8, 128)\n",
      "(256, 1, 1, 128)\n",
      "(256,)\n",
      "(1, 12, 8, 256)\n",
      "(1, 3, 3, 256)\n",
      "(256,)\n",
      "(1, 12, 8, 256)\n",
      "(256, 1, 1, 256)\n",
      "(256,)\n",
      "(1, 12, 8, 256)\n",
      "(1, 3, 3, 256)\n",
      "(256,)\n",
      "(1, 6, 4, 256)\n",
      "(512, 1, 1, 256)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(1, 3, 3, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(512, 1, 1, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(1, 3, 3, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(512, 1, 1, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(1, 3, 3, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(512, 1, 1, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(1, 3, 3, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(512, 1, 1, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(1, 3, 3, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(512, 1, 1, 512)\n",
      "(512,)\n",
      "(1, 6, 4, 512)\n",
      "(1, 3, 3, 512)\n",
      "(512,)\n",
      "(1, 3, 2, 512)\n",
      "(1024, 1, 1, 512)\n",
      "(1024,)\n",
      "(1, 3, 2, 1024)\n",
      "(1, 3, 3, 1024)\n",
      "(1024,)\n",
      "(1, 3, 2, 1024)\n",
      "(1024, 1, 1, 1024)\n",
      "(1024,)\n",
      "(1, 3, 2, 1024)\n",
      "(2,)\n",
      "(1, 1, 1, 1024)\n",
      "(521, 1024)\n",
      "(521,)\n",
      "(1, 521)\n",
      "(1, 521)\n",
      "(1, 521)\n",
      "(4,)\n",
      "(2,)\n",
      "(1024,)\n",
      "(18,)\n",
      "(256,)\n",
      "(1, 48, 32, 9)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(tensors)):\n",
    "    tensor = tensors[i]\n",
    "    print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waveform_binary\n",
      "stft/frame/zeros_like\n",
      "stft/frame/concat\n",
      "stft/frame/ones_like\n",
      "stft/frame/StridedSlice\n",
      "stft/frame/concat_1\n",
      "stft/frame/Reshape\n",
      "stft/frame/add_1\n",
      "stft/frame/GatherV2;stft/frame/GatherV2/axis\n",
      "stft/frame/concat_2/values_1\n",
      "stft/frame/Reshape_3\n",
      "stft/hann_window/sub_2\n",
      "stft/mul\n",
      "stft/rfft/Pad/paddings\n",
      "stft/rfft/Pad\n",
      "stft/rfft1\n",
      "stft/rfft3\n",
      "stft/rfft\n",
      "stft/rfft4\n",
      "stft/rfft2\n",
      "stft/rfft5\n",
      "magnitude_spectrogram\n",
      "mel_spectrogram\n",
      "add\n",
      "mel_spectrogram;add\n",
      "log_mel_spectrogram\n",
      "Reshape/shape\n",
      "feature_patch\n",
      "tfl.quantize\n",
      "ExpandDims\n",
      "ExpandDims1\n",
      "pre_tower/split/split_dim\n",
      "pre_tower/split\n",
      "tower0/network/layer1/conv/Conv2D\n",
      "tower0/network/layer1/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer1/conv/Relu6;tower0/network/layer1/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer2/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer2/sepconv/depthwise;tower0/network/layer1/conv/Conv2D\n",
      "tower0/network/layer2/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer2/sepconv/depthwise\n",
      "tower0/network/layer2/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer2/sepconv/Relu6;tower0/network/layer2/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer2/sepconv/depthwise\n",
      "tower0/network/layer3/conv/Conv2D\n",
      "tower0/network/layer3/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer3/conv/Relu6;tower0/network/layer3/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer4/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer4/sepconv/depthwise;tower0/network/layer3/conv/Conv2D\n",
      "tower0/network/layer4/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer4/sepconv/depthwise\n",
      "tower0/network/layer4/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer4/sepconv/Relu6;tower0/network/layer4/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer4/sepconv/depthwise\n",
      "tower0/network/layer5/conv/Conv2D\n",
      "tower0/network/layer5/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer5/conv/Relu6;tower0/network/layer5/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/depthwise;tower0/network/layer5/conv/Conv2D\n",
      "tower0/network/layer6/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer6/sepconv/depthwise;tower0/network/layer8/sepconv/depthwise\n",
      "tower0/network/layer6/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer6/sepconv/Relu6;tower0/network/layer6/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/depthwise;tower0/network/layer6/sepconv/depthwise\n",
      "tower0/network/layer7/conv/Conv2D\n",
      "tower0/network/layer7/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer7/conv/Relu6;tower0/network/layer7/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/depthwise;tower0/network/layer7/conv/Conv2D\n",
      "tower0/network/layer8/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/depthwise\n",
      "tower0/network/layer8/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer8/sepconv/Relu6;tower0/network/layer8/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer8/sepconv/depthwise\n",
      "tower0/network/layer9/conv/Conv2D\n",
      "tower0/network/layer9/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer9/conv/Relu6;tower0/network/layer9/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/depthwise;tower0/network/layer9/conv/Conv2D\n",
      "tower0/network/layer10/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer10/sepconv/depthwise;tower0/network/layer12/sepconv/depthwise\n",
      "tower0/network/layer10/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer10/sepconv/Relu6;tower0/network/layer10/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/depthwise;tower0/network/layer10/sepconv/depthwise\n",
      "tower0/network/layer11/conv/Conv2D\n",
      "tower0/network/layer11/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer11/conv/Relu6;tower0/network/layer11/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/depthwise;tower0/network/layer11/conv/Conv2D\n",
      "tower0/network/layer12/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/depthwise\n",
      "tower0/network/layer12/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer12/sepconv/Relu6;tower0/network/layer12/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer12/sepconv/depthwise\n",
      "tower0/network/layer13/conv/Conv2D\n",
      "tower0/network/layer13/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer13/conv/Relu6;tower0/network/layer13/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer13/conv/Conv2D\n",
      "tower0/network/layer14/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer14/sepconv/depthwise;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer14/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer14/sepconv/Relu6;tower0/network/layer14/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer14/sepconv/depthwise\n",
      "tower0/network/layer15/conv/Conv2D\n",
      "tower0/network/layer15/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer15/conv/Relu6;tower0/network/layer15/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer15/conv/Conv2D\n",
      "tower0/network/layer16/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer16/sepconv/depthwise;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer16/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer16/sepconv/Relu6;tower0/network/layer16/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer16/sepconv/depthwise\n",
      "tower0/network/layer17/conv/Conv2D\n",
      "tower0/network/layer17/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer17/conv/Relu6;tower0/network/layer17/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer17/conv/Conv2D\n",
      "tower0/network/layer18/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer18/sepconv/depthwise;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer18/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer18/sepconv/Relu6;tower0/network/layer18/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer18/sepconv/depthwise\n",
      "tower0/network/layer19/conv/Conv2D\n",
      "tower0/network/layer19/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer19/conv/Relu6;tower0/network/layer19/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer19/conv/Conv2D\n",
      "tower0/network/layer20/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer20/sepconv/depthwise;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer20/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer20/sepconv/Relu6;tower0/network/layer20/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer20/sepconv/depthwise\n",
      "tower0/network/layer21/conv/Conv2D\n",
      "tower0/network/layer21/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer21/conv/Relu6;tower0/network/layer21/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer21/conv/Conv2D\n",
      "tower0/network/layer22/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer22/sepconv/depthwise;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer22/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer22/sepconv/Relu6;tower0/network/layer22/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer22/sepconv/depthwise\n",
      "tower0/network/layer23/conv/Conv2D\n",
      "tower0/network/layer23/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer23/conv/Relu6;tower0/network/layer23/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise;tower0/network/layer23/conv/Conv2D\n",
      "tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer24/sepconv/Relu6;tower0/network/layer24/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer24/sepconv/depthwise\n",
      "tower0/network/layer25/conv/Conv2D\n",
      "tower0/network/layer25/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer25/conv/Relu6;tower0/network/layer25/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer26/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer26/sepconv/depthwise;tower0/network/layer27/conv/Conv2D;tower0/network/layer25/conv/Conv2D\n",
      "tower0/network/layer26/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer26/sepconv/depthwise;tower0/network/layer27/conv/Conv2D\n",
      "tower0/network/layer26/sepconv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer26/sepconv/Relu6;tower0/network/layer26/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer26/sepconv/depthwise;tower0/network/layer27/conv/Conv2D\n",
      "tower0/network/layer27/conv/Conv2D\n",
      "tower0/network/layer27/conv/BatchNorm/FusedBatchNormV3\n",
      "tower0/network/layer27/conv/Relu6;tower0/network/layer27/conv/BatchNorm/FusedBatchNormV3;tower0/network/layer26/sepconv/BatchNorm/FusedBatchNormV3;tower0/network/layer26/sepconv/depthwise;tower0/network/layer27/conv/Conv2D\n",
      "tower0/network/layer28/reduce_mean/reduction_indices\n",
      "tower0/network/layer28/reduce_mean\n",
      "tower0/network/layer29/fc/MatMul\n",
      "network/layer29/fc/biases\n",
      "tower0/network/layer29/fc/MatMul;tower0/network/layer29/fc/BiasAdd\n",
      "tower0/network/layer32/final_output1\n",
      "tower0/network/layer32/final_output\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for d in ye_deets:\n",
    "    print(d['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\n",
    "base_data_path = './datasets/ESC-50-master/audio/'\n",
    "\n",
    "pd_data = pd.read_csv(esc50_csv)\n",
    "pd_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_classes = ['dog', 'cat']\n",
    "map_class_to_id = {'dog':0, 'cat':1}\n",
    "\n",
    "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
    "\n",
    "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
    "filtered_pd = filtered_pd.assign(target=class_id)\n",
    "\n",
    "full_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
    "filtered_pd = filtered_pd.assign(filename=full_path)\n",
    "\n",
    "filtered_pd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = filtered_pd['filename']\n",
    "targets = filtered_pd['target']\n",
    "folds = filtered_pd['fold']\n",
    "\n",
    "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "  \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "  file_contents = tf.io.read_file(filename)\n",
    "  wav, sample_rate = tf.audio.decode_wav(\n",
    "        file_contents,\n",
    "        desired_channels=1)\n",
    "  wav = tf.squeeze(wav, axis=-1)\n",
    "  sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "  wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "  return wav\n",
    "\n",
    "@tf.function\n",
    "def frame_16k_mono(filename):\n",
    "  wav = load_wav_16k_mono(filename)\n",
    "  frames = tf.signal.frame(wav, 15600, 15600)\n",
    "  return frames\n",
    "\n",
    "    \n",
    "def load_frames_for_map(filename, label, fold):\n",
    "  frames = frame_16k_mono(filename)\n",
    "  return (\n",
    "    frames,\n",
    "    label,\n",
    "    fold\n",
    "  )\n",
    "\n",
    "main_ds = main_ds.map(load_frames_for_map)\n",
    "main_ds.element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unbatch_frames(frames, label, fold):\n",
    "    # num_frames = reduce((lambda x, y: x* y), frames.shape[0:-1])\n",
    "    num_frames = 5\n",
    "    frames = tf.reshape(frames,[num_frames, 15600])\n",
    "    return (\n",
    "        frames, \n",
    "        tf.repeat(label, num_frames),\n",
    "        tf.repeat(fold, num_frames)\n",
    "    )\n",
    "    \n",
    "main_ds = main_ds.map(unbatch_frames).unbatch()\n",
    "main_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data\n",
    "cached_ds = main_ds.cache()\n",
    "train_ds = cached_ds.filter(lambda frame, label, fold: fold < 4)\n",
    "val_ds = cached_ds.filter(lambda frame, label, fold: fold == 4)\n",
    "test_ds = cached_ds.filter(lambda frame, label, fold: fold == 5)\n",
    "\n",
    "# remove the folds column now that it's not needed anymore\n",
    "remove_fold_column = lambda frame, label, fold: (frame, label)\n",
    "\n",
    "train_ds = train_ds.map(remove_fold_column)\n",
    "val_ds = val_ds.map(remove_fold_column)\n",
    "test_ds = test_ds.map(remove_fold_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_ds.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization of weights and activations\n",
    "def representative_dataset():\n",
    "    for frame, label in train_ds.take(100):\n",
    "        yield [frame]\n",
    "\n",
    "list(train_ds.take(1))\n",
    "\n",
    "# next(representative_dataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(yamnet)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model.\n",
    "with open('./models/3/quant_test.tflite', 'wb') as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to isolate just the cnn\n",
    "dummy_input = layers.Input(\n",
    "    batch_shape=(1, 96, 64),\n",
    "    dtype=tf.float32,\n",
    "    name=\"dummy input\"\n",
    ")\n",
    "# r1(dummy_input)\n",
    "cnn = tf.keras.Sequential(\n",
    "    [\n",
    "        dummy_input,\n",
    "        r1\n",
    "    ],\n",
    "    # name=\"yamnet_cnn\",\n",
    "    # inputs=dummy_input,\n",
    "    # outputs=[logits]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tflite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(yamnet)\n",
    "tflite_model = converter.convert()\n",
    "# save to file called me/yamnet.tflite\n",
    "# with open('models/3/me/yamnet_quant.tflite', 'wb') as f:\n",
    "with open('models/3/me/features.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "062744ba756b440054f685d9f8f630c269074acb09f79db40ed3ef0e5e69481c"
  },
  "kernelspec": {
   "display_name": "Python 3.6.15 ('audio3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
