{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changed model input from dynamic to fixed 15600 samples using this: https://medium.com/@antonyharfield/converting-the-yamnet-audio-detection-model-for-tensorflow-lite-inference-43d049bd357c\n",
    "# cloned aheartman's repo instead of the tensorflow model repo like he recommends in the Medium article\n",
    "\n",
    "# transfer learned using this: https://www.tensorflow.org/tutorials/audio/transfer_learning_audio\n",
    "\n",
    "# quantized using this: https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only\n",
    "\n",
    "# compiled to edge tflite using this: https://coral.ai/docs/edgetpu/compiler/#usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_io as tfio\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load fixed input model created in aheartman's repo\n",
    "yamnet_model = tf.keras.models.load_model(\"models/yamnet/tf\")\n",
    "yamnet_model2 = hub.load(\"https://tfhub.dev/google/yamnet/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test_data\\miaow_16k.wav\n"
     ]
    }
   ],
   "source": [
    "testing_wav_file_name = tf.keras.utils.get_file(\n",
    "\t'miaow_16k.wav',\n",
    "\t'https://storage.googleapis.com/audioset/miaow_16k.wav',\n",
    "\tcache_dir='./',\n",
    "\tcache_subdir='test_data'\n",
    ")\n",
    "\n",
    "print(testing_wav_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for loading audio files and making sure the sample rate is correct.\n",
    "\n",
    "@tf.function\n",
    "def load_wav_16k_mono(filename):\n",
    "    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n",
    "    file_contents = tf.io.read_file(filename)\n",
    "    wav, sample_rate = tf.audio.decode_wav(\n",
    "\t\tfile_contents,\n",
    "\t\tdesired_channels=1\n",
    "\t)\n",
    "    wav = tf.squeeze(wav, axis=-1)\n",
    "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
    "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n",
      "Child speech, kid speaking\n",
      "Conversation\n",
      "Narration, monologue\n",
      "Babbling\n",
      "Speech synthesizer\n",
      "Shout\n",
      "Bellow\n",
      "Whoop\n",
      "Yell\n",
      "Children shouting\n",
      "Screaming\n",
      "Whispering\n",
      "Laughter\n",
      "Baby laughter\n",
      "Giggle\n",
      "Snicker\n",
      "Belly laugh\n",
      "Chuckle, chortle\n",
      "Crying, sobbing\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "class_map_path = \"models/yamnet/tf/assets/yamnet_class_map.csv\"\n",
    "class_names =list(pd.read_csv(class_map_path)['display_name'])\n",
    "\n",
    "for name in class_names[:20]:\n",
    "  print(name)\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-1.8715975e-08  6.4694795e-08 -1.3643448e-07 ...  2.4044040e-01\n",
      "  1.7158213e-01  4.3755710e-02], shape=(15600,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.8715975e-08  6.4694795e-08 -1.3643448e-07 ...  2.4044040e-01\n",
      "   1.7158213e-01  4.3755710e-02]], shape=(1, 15600), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n",
    "print(testing_wav_data[0:15600])\n",
    "print(tf.reshape(testing_wav_data[0:15600],(1,15600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main sound is: Animal\n",
      "The embeddings shape: (13, 1024)\n"
     ]
    }
   ],
   "source": [
    "# scores, other = yamnet_model(tf.reshape(testing_wav_data[15600:31200],(1,15600)))\n",
    "scores, embeddings, spectrogram = yamnet_model2(testing_wav_data)\n",
    "class_scores = tf.reduce_mean(scores, axis=0)\n",
    "top_class = tf.argmax(class_scores)\n",
    "inferred_class = class_names[top_class]\n",
    "\n",
    "print(f'The main sound is: {inferred_class}')\n",
    "print(f'The embeddings shape: {embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67 93 81 94 96]\n",
      "The main sound is: Animal\n",
      "The main sound is: Fowl\n",
      "The main sound is: Livestock, farm animals, working animals\n",
      "The main sound is: Chicken, rooster\n",
      "The main sound is: Crowing, cock-a-doodle-doo\n"
     ]
    }
   ],
   "source": [
    "top5 = np.argsort(class_scores)[::-1][:5]\n",
    "print(top5)\n",
    "for i in top5:\n",
    "\tinferred_class = class_names[i]\n",
    "\tprint(f'The main sound is: {inferred_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-2.688576   -3.395819   -3.3842807  ... -1.9334265  -1.6974654\n",
      "  -1.9440112 ]\n",
      " [-1.7409291  -1.591264   -1.7010896  ... -2.055423   -1.6811252\n",
      "  -1.8205918 ]\n",
      " [-1.6530831  -2.106609   -3.3813334  ... -2.2581878  -2.081408\n",
      "  -1.6031291 ]\n",
      " ...\n",
      " [-2.439605   -1.66035    -1.6649653  ... -1.3464409  -1.5554621\n",
      "  -1.2106203 ]\n",
      " [-2.2034495  -3.5700097  -2.8465383  ... -1.5448197  -1.2740288\n",
      "  -1.0537565 ]\n",
      " [-2.8868978  -2.0260997  -1.8647212  ... -1.101402   -0.80602986\n",
      "  -1.2401828 ]], shape=(96, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_arg_keywords', '_as_name_attr_list', '_attrs', '_build_call_outputs', '_call_flat', '_call_impl', '_call_with_flat_signature', '_call_with_structured_signature', '_captured_closures', '_captured_inputs', '_delayed_rewrite_functions', '_experimental_with_cancellation_manager', '_filtered_call', '_first_order_tape_functions', '_flat_signature_summary', '_func_graph', '_function_spec', '_garbage_collector', '_get_gradient_function', '_higher_order_tape_functions', '_inference_function', '_initialize_function_spec', '_ndarray_singleton', '_ndarrays_list', '_num_positional_args', '_output_shapes', '_pre_initialized_function_spec', '_select_forward_and_backward_functions', '_self_saveable_object_factories', '_set_function_spec', '_structured_signature_check_arg_type', '_structured_signature_check_arg_types', '_structured_signature_check_missing_args', '_structured_signature_check_unexpected_args', '_structured_signature_summary', 'add_gradient_functions_to_graph', 'add_to_graph', 'captured_inputs', 'function_def', 'graph', 'inputs', 'name', 'output_dtypes', 'output_shapes', 'outputs', 'pretty_printed_signature', 'structured_input_signature', 'structured_outputs', 'trainable_variables', 'variables']\n"
     ]
    }
   ],
   "source": [
    "print(dir(yamnet_model2.signatures['serving_default']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7413fcc3059bb90f4a23006199db059287bed52888a62c3ff9499ab51341827d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('audio')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
